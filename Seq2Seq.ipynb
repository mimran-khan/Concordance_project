{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max suffix length: 24\n",
      "Vocabulary size: 39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\prana\\\\Downloads\\\\Nouns_new_6.xlsx\")\n",
    "\n",
    "# Prepare input and target texts\n",
    "input_texts = df['Word'].astype(str).tolist()\n",
    "target_texts = df['Base Word'].astype(str).tolist()\n",
    "\n",
    "# Extract suffixes (exclude the first character)\n",
    "input_suffixes = [word[1:] for word in input_texts]\n",
    "target_suffixes = [base_word[1:] for base_word in target_texts]\n",
    "\n",
    "# Combine all characters from input and target suffixes to create a tokenizer\n",
    "all_texts = input_suffixes + target_suffixes\n",
    "\n",
    "# Character-level tokenization\n",
    "tokenizer = Tokenizer(char_level=True, filters='')\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "input_sequences = tokenizer.texts_to_sequences(input_suffixes)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_suffixes)\n",
    "\n",
    "# Determine the maximum sequence length for padding\n",
    "max_suffix_length = max(len(seq) for seq in input_sequences)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_suffix_length, padding='post')\n",
    "decoder_input_data = pad_sequences(target_sequences, maxlen=max_suffix_length, padding='post')\n",
    "\n",
    "# Prepare the target data, shifted by one timestep\n",
    "decoder_target_data = np.zeros_like(decoder_input_data)\n",
    "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
    "decoder_target_data[:, -1] = 0  # Padding for the last timestep\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Print some statistics\n",
    "print(f'Max suffix length: {max_suffix_length}')\n",
    "print(f'Vocabulary size: {vocab_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 24, 64)            2496      \n",
      "                                                                 \n",
      " positional_encoding (Posit  (None, 24, 64)            1536      \n",
      " ionalEncoding)                                                  \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 24, 64)            83200     \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " transformer_block_1 (Trans  (None, 24, 64)            83200     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_2 (Trans  (None, 24, 64)            83200     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " transformer_block_3 (Trans  (None, 24, 64)            83200     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 24, 39)            2535      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 339367 (1.29 MB)\n",
      "Trainable params: 339367 (1.29 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization, Dropout, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define hyperparameters\n",
    "embedding_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128  # Feed forward network dimension\n",
    "num_layers = 4\n",
    "\n",
    "# Define the transformer block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_emb = self.add_weight(\"pos_emb\", shape=[maxlen, embed_dim])\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_emb\n",
    "\n",
    "# Define the model\n",
    "def build_transformer_model(vocab_size, maxlen, embed_dim, num_heads, ff_dim, num_layers):\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    x = Embedding(vocab_size, embed_dim)(inputs)\n",
    "    x = PositionalEncoding(maxlen, embed_dim)(x)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Build and compile the model\n",
    "transformer_model = build_transformer_model(vocab_size, max_suffix_length, embedding_dim, num_heads, ff_dim, num_layers)\n",
    "transformer_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 19s 191ms/step - loss: 1.0921 - accuracy: 0.8136 - val_loss: 0.5535 - val_accuracy: 0.8952\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 3s 142ms/step - loss: 0.5341 - accuracy: 0.8800 - val_loss: 0.3644 - val_accuracy: 0.8962\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 3s 145ms/step - loss: 0.4405 - accuracy: 0.8860 - val_loss: 0.3426 - val_accuracy: 0.9208\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 7s 342ms/step - loss: 0.3808 - accuracy: 0.8943 - val_loss: 0.3326 - val_accuracy: 0.9207\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 7s 328ms/step - loss: 0.3300 - accuracy: 0.9071 - val_loss: 0.3311 - val_accuracy: 0.9281\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 6s 287ms/step - loss: 0.2811 - accuracy: 0.9189 - val_loss: 0.3226 - val_accuracy: 0.9254\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 6s 315ms/step - loss: 0.2191 - accuracy: 0.9398 - val_loss: 0.3095 - val_accuracy: 0.9234\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 5s 234ms/step - loss: 0.1645 - accuracy: 0.9565 - val_loss: 0.2935 - val_accuracy: 0.9362\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 5s 228ms/step - loss: 0.1228 - accuracy: 0.9702 - val_loss: 0.3135 - val_accuracy: 0.9355\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 5s 237ms/step - loss: 0.0943 - accuracy: 0.9790 - val_loss: 0.3155 - val_accuracy: 0.9355\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 5s 240ms/step - loss: 0.0777 - accuracy: 0.9825 - val_loss: 0.3197 - val_accuracy: 0.9385\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0608 - accuracy: 0.9861 - val_loss: 0.3326 - val_accuracy: 0.9336\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 4s 195ms/step - loss: 0.0492 - accuracy: 0.9886 - val_loss: 0.3475 - val_accuracy: 0.9322\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 4s 183ms/step - loss: 0.0380 - accuracy: 0.9915 - val_loss: 0.3568 - val_accuracy: 0.9378\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 4s 218ms/step - loss: 0.0315 - accuracy: 0.9933 - val_loss: 0.3682 - val_accuracy: 0.9275\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 5s 266ms/step - loss: 0.0267 - accuracy: 0.9940 - val_loss: 0.3580 - val_accuracy: 0.9346\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 4s 206ms/step - loss: 0.0217 - accuracy: 0.9955 - val_loss: 0.3515 - val_accuracy: 0.9344\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 4s 205ms/step - loss: 0.0230 - accuracy: 0.9947 - val_loss: 0.3836 - val_accuracy: 0.9306\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 5s 231ms/step - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.3868 - val_accuracy: 0.9189\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 5s 268ms/step - loss: 0.0157 - accuracy: 0.9967 - val_loss: 0.3635 - val_accuracy: 0.9314\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 4s 196ms/step - loss: 0.0119 - accuracy: 0.9974 - val_loss: 0.3886 - val_accuracy: 0.9281\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 5s 243ms/step - loss: 0.0106 - accuracy: 0.9979 - val_loss: 0.4033 - val_accuracy: 0.9279\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 4s 217ms/step - loss: 0.0099 - accuracy: 0.9978 - val_loss: 0.3674 - val_accuracy: 0.9292\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 4s 188ms/step - loss: 0.0085 - accuracy: 0.9982 - val_loss: 0.4238 - val_accuracy: 0.9253\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 4s 195ms/step - loss: 0.0088 - accuracy: 0.9981 - val_loss: 0.4100 - val_accuracy: 0.9277\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 4s 220ms/step - loss: 0.0084 - accuracy: 0.9982 - val_loss: 0.3914 - val_accuracy: 0.9289\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.4257 - val_accuracy: 0.9254\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 0.0068 - accuracy: 0.9986 - val_loss: 0.3736 - val_accuracy: 0.9301\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 4s 196ms/step - loss: 0.0069 - accuracy: 0.9984 - val_loss: 0.3812 - val_accuracy: 0.9307\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 6s 306ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.4122 - val_accuracy: 0.9277\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 7s 376ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.4210 - val_accuracy: 0.9275\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 5s 250ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.4216 - val_accuracy: 0.9294\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 4s 226ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.4077 - val_accuracy: 0.9326\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.4300 - val_accuracy: 0.9247\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 5s 235ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.4487 - val_accuracy: 0.9219\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 5s 239ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.4285 - val_accuracy: 0.9314\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.4149 - val_accuracy: 0.9264\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 4s 193ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.3787 - val_accuracy: 0.9327\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 4s 207ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.4255 - val_accuracy: 0.9292\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.4126 - val_accuracy: 0.9306\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 4s 190ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.4084 - val_accuracy: 0.9358\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 4s 184ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.4206 - val_accuracy: 0.9293\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 4s 200ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.4821 - val_accuracy: 0.9276\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 4s 217ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.4591 - val_accuracy: 0.9268\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 7s 344ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.4438 - val_accuracy: 0.9303\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 5s 268ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.4268 - val_accuracy: 0.9320\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 4s 220ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.4439 - val_accuracy: 0.9290\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.4372 - val_accuracy: 0.9340\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 4s 186ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.4342 - val_accuracy: 0.9319\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 4s 198ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.4380 - val_accuracy: 0.9323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x268c03a9890>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "transformer_model.fit(encoder_input_data, decoder_target_data,\n",
    "                      batch_size=64, epochs=50, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Input: pranavaya\n",
      "Predicted Base Word: paá¹£n\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq, first_char):\n",
    "    # Predict the output sequence (suffix)\n",
    "    output_seq = transformer_model.predict(input_seq)\n",
    "    # Decode the sequence into characters\n",
    "    decoded_suffix = ''.join([tokenizer.index_word.get(np.argmax(char_prob), '') for char_prob in output_seq[0]])\n",
    "    # Reconstruct the full base word by prepending the first character\n",
    "    decoded_word = first_char + decoded_suffix.strip()\n",
    "    return decoded_word\n",
    "\n",
    "# Test with a new word\n",
    "test_input = \"krishnaya\"  # Replace with any new word\n",
    "first_char = test_input[0]\n",
    "test_input_suffix = test_input[1:]\n",
    "test_input_seq = pad_sequences(tokenizer.texts_to_sequences([test_input_suffix]), maxlen=max_suffix_length, padding='post')\n",
    "\n",
    "# Predict the base word\n",
    "predicted_base_word = decode_sequence(test_input_seq, first_char)\n",
    "print(f'Input: {test_input}')\n",
    "print(f'Predicted Base Word: {predicted_base_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
